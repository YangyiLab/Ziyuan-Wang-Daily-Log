- [2021-11-1](#2021-11-1)
  - [PLAN](#plan)
  - [进入系统填报信息](#进入系统填报信息)
    - [ietls number](#ietls-number)
    - [Penn State](#penn-state)
    - [MADISON](#madison)
  - [无植物微生物结论基调](#无植物微生物结论基调)
    - [微生物不同氮肥浓度](#微生物不同氮肥浓度)
    - [不同氮肥浓度是否引发hitchhiking](#不同氮肥浓度是否引发hitchhiking)
  - [修改甲基化文件 转成bed](#修改甲基化文件-转成bed)
- [2021-11-2](#2021-11-2)
  - [PLAN](#plan-1)
  - [deep learning approach antibiotics](#deep-learning-approach-antibiotics)
- [2021-11-3](#2021-11-3)
  - [PLAN](#plan-2)
  - [机器学习作业总结](#机器学习作业总结)
    - [loop 固定写法](#loop-固定写法)
    - [全连接模型](#全连接模型)
    - [卷积](#卷积)
    - [卷积实现相关参数](#卷积实现相关参数)
- [2021-11-4](#2021-11-4)
  - [PLAN](#plan-3)
  - [微生物](#微生物)
  - [卷积神经网络](#卷积神经网络)
    - [resnet](#resnet)
    - [卷积变种](#卷积变种)
- [2021-11-5](#2021-11-5)
  - [PLAN](#plan-4)
  - [resnet实现代码](#resnet实现代码)
  - [微生物introduction 修改](#微生物introduction-修改)
    - [原introduction 逻辑](#原introduction-逻辑)
    - [修改逻辑](#修改逻辑)
  - [卷积神经网络训练方法 batch noemalization](#卷积神经网络训练方法-batch-noemalization)
- [2021-11-6](#2021-11-6)
  - [PLAN](#plan-5)
  - [Penn State 问题](#penn-state-问题)
  - [UA账户](#ua账户)
  - [单细胞论文](#单细胞论文)
    - [结论](#结论)
    - [方法](#方法)
  - [卷积神经网络模块细节](#卷积神经网络模块细节)
    - [全局平均池化](#全局平均池化)
    - [瓶颈模型](#瓶颈模型)
  - [进一步关注文章](#进一步关注文章)
- [2021-11-7](#2021-11-7)
  - [PLAN](#plan-6)
  - [Penn State 问题](#penn-state-问题-1)
  - [Graph Embedding](#graph-embedding)
    - [Deep work](#deep-work)
- [2021-11-7](#2021-11-7-1)
  - [PLAN](#plan-7)
  - [Diversity Statement](#diversity-statement)
- [2021-11-8](#2021-11-8)
  - [PLAN](#plan-8)
- [2021-11-9](#2021-11-9)
  - [PLAN](#plan-9)
  - [Introduction 内容](#introduction-内容)
  - [矩阵求导](#矩阵求导)
- [2021-11-11](#2021-11-11)
  - [PLAN](#plan-10)
  - [注释方法](#注释方法)
    - [比较基因组方法 mauve](#比较基因组方法-mauve)
    - [Steps](#steps)
  - [introduction](#introduction)
  - [RNN](#rnn)
    - [GRU](#gru)
    - [LSTM](#lstm)
- [2021-11-12](#2021-11-12)
  - [PLAN](#plan-11)
  - [玻尔兹曼机](#玻尔兹曼机)
  - [清洗数据](#清洗数据)
- [2021-11-13](#2021-11-13)
  - [PLAN](#plan-12)
  - [VAE 项目变量解读](#vae-项目变量解读)
    - [进度](#进度)
    - [模型搭建完毕](#模型搭建完毕)
  - [文章讨论](#文章讨论)
    - [对于无氮肥现象](#对于无氮肥现象)
    - [三者关系](#三者关系)
  - [蛋白建模扩展](#蛋白建模扩展)
    - [wiki资料](#wiki资料)
    - [文献](#文献)
- [2021-11-14](#2021-11-14)
  - [PLAN](#plan-13)
  - [VAE 损失函数](#vae-损失函数)
  - [调参](#调参)
- [2021-11-15](#2021-11-15)
  - [PLAN](#plan-14)
  - [Problems](#problems)
  - [perturb-seq](#perturb-seq)
    - [文章符号](#文章符号)
- [2021-11-16](#2021-11-16)
  - [PLAN](#plan-15)
  - [单细胞](#单细胞)
    - [代码备注](#代码备注)
    - [umap 图](#umap-图)
    - [需解决问题](#需解决问题)
- [2021-11-17](#2021-11-17)
  - [PLAN](#plan-16)
  - [VAE 不考虑tf数据后恢复情况](#vae-不考虑tf数据后恢复情况)
- [2021-11-18](#2021-11-18)
  - [PLAN](#plan-17)
  - [多利用scvi的代码为基础](#多利用scvi的代码为基础)
  - [宏基因组参考文献](#宏基因组参考文献)
- [2021-11-19](#2021-11-19)
  - [PLAN](#plan-18)
  - [scanpy 数据结构总结](#scanpy-数据结构总结)
    - [对于该项目](#对于该项目)
  - [VAE调参](#vae调参)
  - [功能](#功能)
    - [无植物](#无植物)
    - [有植物](#有植物)
  - [讨论](#讨论)
  - [无植物微生物结论基调](#无植物微生物结论基调-1)
    - [微生物不同氮肥浓度](#微生物不同氮肥浓度-1)
- [2021-11-20](#2021-11-20)
  - [PLAN](#plan-19)
  - [训练记录](#训练记录)
  - [文章](#文章)
    - [2021年applied](#2021年applied)
    - [2021 fim](#2021-fim)
- [2021-11-21](#2021-11-21)
  - [PLAN](#plan-20)
  - [大流行的影响 文件](#大流行的影响-文件)
  - [宏基因组文章3](#宏基因组文章3)
    - [主线逻辑](#主线逻辑)
    - [方法](#方法-1)
- [2021-11-22](#2021-11-22)
  - [PLAN](#plan-21)
  - [宏基因组方法](#宏基因组方法)
  - [umap 数据](#umap-数据)
- [2021-11-23](#2021-11-23)
  - [PLAN](#plan-22)
- [2021-11-24](#2021-11-24)
  - [PLAN](#plan-23)
  - [申请情况总结](#申请情况总结)
    - [Madison](#madison-1)
    - [Penn State](#penn-state-1)
    - [Arizona](#arizona)
    - [Tulane](#tulane)
    - [ND](#nd)
    - [UNC](#unc)
    - [CMU](#cmu)
  - [配置ltr_finder](#配置ltr_finder)
- [2021-11-25](#2021-11-25)
  - [PLAN](#plan-24)
  - [论文中代码指导(NAR)](#论文中代码指导nar)
- [11.26 度过了美好的感恩节假期](#1126-度过了美好的感恩节假期)
- [2021-11-27](#2021-11-27)
  - [PLAN](#plan-25)
- [2021-11-28](#2021-11-28)
  - [PLAN](#plan-26)

# 2021-11-1
## PLAN
+ **修改PS**
+ **微生物作图**
+ **进入系统填报信息**
+ **修改甲基化文件 转成bed**

## 进入系统填报信息
    
### ietls number
20CN002445WANZ001A


### Penn State
+ 地址 No.29 Wangjiang Road, Chengdu, Sichuan, China,610064
+ 密码 wzy851234wzy851234
+ 登录号 zxw5399@psu.edu

### MADISON
+ 密码 5#DvMnQwAwHhL+r
+ user name pry0921

## 无植物微生物结论基调

### 微生物不同氮肥浓度
**H1-F-vs-H2-F-vs-H3-F 0.8601 0.006 ** 无差异**
H1-NF-vs-H2-NF-vs-H3-NF 0.3333 0.026 *

### 不同氮肥浓度是否引发hitchhiking
H1-NF H1-F 0.343565
H2-NF H2-F 0.08328
H3-NF H3-F 0.001832 **

## 修改甲基化文件 转成bed
代码
```python
#   API trans_mCfile2bed
#   trans_mCfile2bed(path,input_file_name)
#   path 文件相应路径 file_name 甲基化文件名称

def judge_if_mC(line):
    if str(line).endswith("1"):
        return True
    return False

def trans2bed(line):
    line_list=line.split("\t")
    chr_num="chr"+line_list[0]
    start_position=line_list[1]
    end_position=str(int(start_position)+1)
    name=line_list[3]
    length_of_mC="1"
    strand=line_list[2]
    linr_out="\t".join([chr_num,start_position,end_position,name,length_of_mC,strand])
    return linr_out

def trans_mCfile2bed(path,input_file_name):
    # path= "/home/ubuntu/Arabidopsis/Arabidopsis_sequence/Ag-0/"
    # input_file_name="GSM1085193_mC_calls_Ag_0.tsv"
    f = open(path+input_file_name,"r")
    mc_data=f.read()
    mc_lines=mc_data.split("\n")
    [judge_if_mC(i) for i in mc_lines]
    mc_true_lines=list(filter(judge_if_mC,mc_lines))
    list(mc_true_lines)
    mc_true_lines_out=[trans2bed(line) for line in mc_true_lines]
    output_file_name=input_file_name[20:-4]+".mC.bed"
    f_bed=open(path+output_file_name,"w")
    f_bed.write("\n".join(mc_true_lines_out))
    output_file_name=input_file_name[20:-4]+".mC.bed"
    return output_file_name
```

# 2021-11-2
## PLAN
+ **微生物功能分析图**
+ **GCN启发论文**

## deep learning approach antibiotics
方法 MPNN 消息传递神经网络 GNN的一种

# 2021-11-3
    
## PLAN
+ **微生物热图**
+ **MINSET数据集overvoew pytorch回忆 基于教材 Dive into**

## 机器学习作业总结
    
### loop 固定写法
当定义好训练模型后，不需要修改train和test loop是固定的，如以下框架
```python
def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")


def test_loop(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
```

### 全连接模型
           
```python
import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
import numpy as np
import torch.nn.functional as F
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits
```

### 卷积
```python
class CoNeuralNetwork(nn.Module):
    def __init__(self):
        super(CoNeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack =  nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.ReLU(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.ReLU(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.ReLU(),
    nn.Linear(120, 84), nn.ReLU(),
    nn.Linear(84, 10))

    def forward(self, x):
        logits = self.linear_relu_stack(x)
        return logits
```

### 卷积实现相关参数
+ kernel 参数 只涉及卷积核形状
+ stride  步幅

在计算互相关时，卷积窗口从输入张量的左上角开始，向下和向右滑动。 在前面的例子中，我们默认每次滑动一个元素。 但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。

![stride](https://zh-v2.d2l.ai/_images/conv-stride.svg)

例子中 **垂直步幅为  3 ，水平步幅为  2  的二维互相关运算**

pytorch实现

```python
## 右下步幅一致情况
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)
## 右下步幅不一致情况
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
```
同时在上面例子中卷积核 padding stride都不同

+ padding 在行和列填充

![padding](https://zh-v2.d2l.ai/_images/conv-pad.svg)

例子中添加了一列一行

pytorch 实现
```python
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)
```
+ output 下一层的数量 决定了卷积数量


# 2021-11-4

## PLAN
+ **拉美研究ppt初稿**
+ **百面前向神经网络**
+ **微生物调参**

## 微生物
H1-F-vs-H2-F-vs-H3-F0.8601 0.006 **
H1-NF-vs-H2-NF-vs-H3-NF 0.3333 0.026 *

## 卷积神经网络

### resnet
核心 每－层都不比上一层差 每一层新网络训练残差

### 卷积变种
反卷积	转置卷积
实例 AE  VAE GAN

# 2021-11-5

## PLAN
+ **论文修改introduction设计**
+ **卷积神经网络训练方法 loss计算overview**
+ **resnet实现 pytorch**

## resnet实现代码

```python
#model.py

import torch.nn as nn
import torch

#18/34
class BasicBlock(nn.Module):
    expansion = 1 #每一个conv的卷积核个数的倍数

    def __init__(self, in_channel, out_channel, stride=1, downsample=None):#downsample对应虚线残差结构
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,
                               kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channel)#BN处理
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,
                               kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channel)
        self.downsample = downsample

    def forward(self, x):
        identity = x #捷径上的输出值
        if self.downsample is not None:
            identity = self.downsample(x)

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        out += identity
        out = self.relu(out)

        return out

#50,101,152
class Bottleneck(nn.Module):
    expansion = 4#4倍

    def __init__(self, in_channel, out_channel, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,
                               kernel_size=1, stride=1, bias=False)  # squeeze channels
        self.bn1 = nn.BatchNorm2d(out_channel)
        self.relu = nn.ReLU(inplace=True)
        # -----------------------------------------
        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,
                               kernel_size=3, stride=stride, bias=False, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channel)
        self.relu = nn.ReLU(inplace=True)
        # -----------------------------------------
        self.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel*self.expansion,#输出*4
                               kernel_size=1, stride=1, bias=False)  # unsqueeze channels
        self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample

    def forward(self, x):
        identity = x
        if self.downsample is not None:
            identity = self.downsample(x)

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        out += identity
        out = self.relu(out)

        return out


class ResNet(nn.Module):

    def __init__(self, block, blocks_num, num_classes=1000, include_top=True):#block残差结构 include_top为了之后搭建更加复杂的网络
        super(ResNet, self).__init__()
        self.include_top = include_top
        self.in_channel = 64

        self.conv1 = nn.Conv2d(1, self.in_channel, kernel_size=7, stride=2,
                               padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(self.in_channel)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, blocks_num[0])
        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)
        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)
        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)
        if self.include_top:
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # output size = (1, 1)自适应
            self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

    def _make_layer(self, block, channel, block_num, stride=1):
        downsample = None
        if stride != 1 or self.in_channel != channel * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(channel * block.expansion))

        layers = []
        layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride))
        self.in_channel = channel * block.expansion

        for _ in range(1, block_num):
            layers.append(block(self.in_channel, channel))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        if self.include_top:
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.fc(x)

        return x


def resnet34(num_classes=1000, include_top=True):
    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)


def resnet101(num_classes=1000, include_top=True):
    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)
```

这套代码可以训练MNSIT

## 微生物introduction 修改

### 原introduction 逻辑
从hitchhiking现象说起，影响hitchhiking有物理化学环境 

氮浓度是直接影响微生物群落结构的关键因素 

### 修改逻辑
hitchhiking现象介绍 影响hitchhiking已有的报道 氮可以影响微生物群落 但不知道是否影响hitchhiking 

另一方面 植物分泌物作为土壤细菌的营养来源 是否促进hitchhiking 

植物和氮对hitchhiking的关系 


## 卷积神经网络训练方法 batch noemalization
加入BN batch Normalization 后 
BN层需要多学习 $\beta$  $\gamma$ 方法 BP

# 2021-11-6

## PLAN
+ **Penn State & UA 材料准备**
+ **阅读单细胞文献1篇**
+ **阅读G4文献1**
+ **卷积神经网络百面**

## Penn State 问题
+ Describe your most meaningful independent research experience. Your answer should include the scientific questions or hypotheses you asked, the experimental design you used to test the hypothesis or question, your results, and the conclusions you drew about the biological processes you studied. Describe your role in the project: Were you part of a research team? What parts of the project were you responsible for? 500 words maximum.

+ Generally, it's considered best practice for a student to move to a new institution for Graduate School. If you are currently a Penn State, University Park campus student, please tell us why staying here is a good choice for you.

+ Briefly describe an academic obstacle you faced or a challenge you have met. What strategies did you use to overcome the challenge? If you faced a future technical problem in your research, how would you overcome this hurdle. 250 words maximum

+ What are your current research interests, why is that field/area attractive to you, and which faculty in the department would allow you to pursue these interests? What do you see yourself doing in 10 years from now? 250 words maximum

+ Socially diverse groups do better science, are more productive and innovative, and make better decisions. Describe any past activities that have supported diversity and inclusion. How would you support or contribute to diversity and inclusion in the Biology program? 250 words maximum

+ Describe something in your academic life that you are most proud of. 250 words maximum.

## UA账户
邮箱 13230859192@163.com
密码 b#UFaXFK千*.7EPc

## 单细胞论文

### 结论
整合 rna-seq atac-seq 等多组学进行降维 节点分类等

我们展示了 SIMBA 提供了一个单一的框架，允许以通用的方式制定不同的单细胞分析问题，从而简化新分析的开发和其他单细胞模式的集成。

### 方法
图嵌入 可以利用 GCN 代替吗？

## 卷积神经网络模块细节

### 全局平均池化

目的：取代全连接层
优势：降低计算量提高可解释性

### 瓶颈模型

利用1*1卷积做升降维	尤其在大卷积（通道较多时）

## 进一步关注文章
+ https://com-mendeley-prod-publicsharing-pdfstore.s3.eu-west-1.amazonaws.com/0524-PUBMED/10.1371/journal.pone.0113955/

+ https://www.sciencedirect.com/science/article/pii/S0006291X20301935#bib22

+ https://doi.org/10.1016/j.bbagen.2018.06.014

# 2021-11-7

## PLAN
+ **Penn State问题回答**
+ **图嵌入学习**

## Penn State 问题
+ Describe your most meaningful independent research experience. Your answer should include the scientific questions or hypotheses you asked, the experimental design you used to test the hypothesis or question, your results, and the conclusions you drew about the biological processes you studied. Describe your role in the project: Were you part of a research team? What parts of the project were you responsible for? 500 words maximum.

今年九月开始，我们实验室开展了一项新课题，关于G4、甲基化和转座子在拟南芥全基因组的研究。这个课题是第一次我独自设计的课题并实施的，目前依然在进行。一些研究报道了G4结构与LTR末端区域的研究，同时也有报道了G4在CpG island处富集，我猜测G4结构和甲基化有关。同时G4潜在序列是否折叠除了受序列因素影响还会受理化性质的影响，对于不同类型的拟南芥在不同的生态环境类型，相同序列是否折叠会出现不同情况，我们基于这种假设对于Genome1001项目中世界各地的拟南芥基因组，进行了全基因组分析。考虑到TAIR10项目中已经有了1135种拟南芥在世界各地的坐标，甲基化注释图谱，以及全基因组序列。我们首先利用软件Quadron (一种基于机器学习预测pqs的软件)识别全部G4结构。之后通过统计，识别出的潜在G4结构周边的甲基化程度，推断是否该潜在G4进行了折叠。综合多个拟南芥类群的信息，我们可以推断哪些类别潜在G4结构是否折叠受地理因素影响较大。同时还利用LTR识别软件，将不同拟南芥类群的LTR识别出来，并判断不同类别的拟南芥存在的LTR差异。如果差异LTR的末端重复序列G4结构附近的甲基化程度很高，这意味着不同地理位置的拟南芥基因组种转座子差异主要是由地理位置对于G4结构折叠影响，从而导致逆转录转座子LTR在基因组中的转座受到影响，导致拟南芥物种分化。

目前我们的发现不同地区分布的生态型，在G4区域附近的甲基化程度不同。同时，我们还提出假设，不同地区的拟南芥类群在转座子层面的差异，主要的原因就是，G4结构受地理因素影响，导致其抑制甲基化程度降低，使得转座子无法转座，这一假设还需要继续结合地理因素进行探究。

考虑到我此前已经有了一定的科研训练基础，我在这个项目中作为负责人带领整个团队，从选题到基因组数据获取，和最终的项目实施以及最终完成paper都将由我主要负责。在我们的团队中还有一些目前大学三年级的同学，我还与这些同学进行合作，共同完成这个课题。

+ Generally, it's considered best practice for a student to move to a new institution for Graduate School. If you are currently a Penn State, University Park campus student, please tell us why staying here is a good choice for you.

+ Briefly describe an academic obstacle you faced or a challenge you have met. What strategies did you use to overcome the challenge? If you faced a future technical problem in your research, how would you overcome this hurdle. 250 words maximum

+ What are your current research interests, why is that field/area attractive to you, and which faculty in the department would allow you to pursue these interests? What do you see yourself doing in 10 years from now? 250 words maximum

目前我对基因组学很感兴趣，包括人类进化与疾病的关系很感兴趣，同时技术层面上我对于机器学习深度学习结合生物模型很感兴趣。此前，我的关于Genome1001拟南芥群体基因组的项目与群体遗传学相关，我在本科的学习大量的生物学、统计学以及编程知识。因此，我将宾州州立大学生物博士项目视为一个理想的平台，以实现我成为群体遗传学领域领先的研究人员的梦想。此外，目前随着基因组技术的发展，通过群体遗传学手段帮助人类治疗治病越来越成为可能，我在一方面进一步的研究不仅可以为人类疾病治疗做贡献，同时在精准医疗行业发展越来越快的情况下，对自己的职业生涯也十分有益。

Dr. Yifei Huang 的研究我十分感兴趣，不管是研究的科学问题(从群体遗传学角度研究自然选择对于人类进化和疾病的影响)，还是所采用的方法(用计算手段与机器学习还有深度学习结合)都十分吸引我，同时受他研究的启发，我将他研究的一些方法用到了我研究拟南芥群体基因组项目中。

在十年后，我想我可能在高校寻求教职，做科学研究，也有可能在医疗公司或者医院做研究人员，对于人类群体疾病进行研究。在这十年中，不仅我希望自己能够发表高水平论文，也希望自己的学术成果能真正帮助到病人，解决一些实际问题。同时也希望能开发方法或者软件，帮助到整个研究领域。

+ Socially diverse groups do better science, are more productive and innovative, and make better decisions. Describe any past activities that have supported diversity and inclusion. How would you support or contribute to diversity and inclusion in the Biology program? 250 words maximum

+ Describe something in your academic life that you are most proud of. 250 words maximum.

## Graph Embedding

作用 ： 如果没有Graph Embedding 要使用one hot编码 但是可能长度过于长，同时失去了节点之间的信息

### Deep work
https://zhuanlan.zhihu.com/p/45167021
参数为 参数矩阵 每一个节点映射的向量
利用 似然函数 做梯度下降
似然函数为 在出现某一结点$v_i$的条件下，出现某一个序列的概率 其中概率部分的意思是，在一个随机游走中，当给定一个顶点 $v_i$时，出现它的w窗口范围内顶点的概率。
训练样本构成 : 通过随机游走，建立大量的路径

# 2021-11-7

## PLAN
+ **图嵌入其他算法、代码**
+ **diversity statement完成**

## Diversity Statement

+ Socially diverse groups do better science, are more productive and innovative, and make better decisions. Describe any past activities that have supported diversity and inclusion. How would you support or contribute to diversity and inclusion in the Biology program? 250 words maximum

It is well known that the United States is a diverse society, and Penn State argues that socially diverse groups do better science, are more productive and innovative, and make better decisions.
In my opinion, I accept this tradition and I think diversity is the key to our evolution because it brings new tastes to our existing communities.
Living in a pluralistic environment, where different cultures and environments bring with them different personal experiences, values and worldviews, helps people interact effectively with each other and prepares them to participate in an increasingly complex and pluralistic society.
Take me for example.
When I was in late primary school, my mother left home to study for a PhD, so I learned to take care of myself and manage my time.
During this time, I learned how to cook and clean my room.
Although the experience was a bit difficult, it improved my self-reliance.
In addition, this special experience also reminds me to have confidence to overcome the difficulties in university study and research work.
After entering Sichuan University, I participated in many volunteer and community activities, such as serving as the volunteer of Chengdu station of Chinese Society of Cell Biology Conference, caring for the elderly in nursing homes, and accompanying the elderly during festivals.
In addition, when I entered the senior year of university, I also began to guide junior students to do projects.
These experiences are my valuable asset.
On the one hand, these activities give me a sense of achievement.
On the other hand, I enjoyed brainstorming and discussing academic issues with my younger brothers and sisters in the process of instructing them, although they might lack knowledge.
These gave me the idea of applying for a PhD, hoping to make more contributions to the society.

# 2021-11-8

## PLAN
+ **填写 Penn State Madison UArizona网申**
+ **networkx overview**


# 2021-11-9

## PLAN
+ **刘老师推荐信**
+ **introduction 文献整理**
+ **矩阵求导 推导**

## Introduction 内容
+ 8种拟南芥种质对与其根相关的细菌产生显着的选择性影响。群落物种组成和相对丰度差异均显着（P<0.001)。八个不同且可重复的加入依赖群落概况也与对照大块土壤不同。这些变体的根系分泌物通过高效液相色谱 (HPLC) 进行分析  根际分泌物改变微生物组 Shirley A. Micallef, Michael P. Shiaris, Adán Colón-Carmona, Influence of Arabidopsis thaliana accessions on rhizobacterial communities and natural variation in root exudates, Journal of Experimental Botany, Volume 60, Issue 6, April 2009, Pages 1729–1742, https://doi.org/10.1093/jxb/erp053

+ 土壤-根系关联的复杂性体现在根际在空间和时间上的组成异质性。这在图1a中进行了说明，该图 显示了穿过普通灯心草 Juncus effusus根和根际的O 2剖面。根系影响土壤 pH 值，如图1b中硬粒小麦 ( Triticum durum ) 和鹰嘴豆 ( Cicer arietinum )的根际所示 。植物根部不断分泌小分子，包括糖和维生素，以及复杂的蛋白质。图1所示的根-土相互作用 是决定根际微生物群落性质的主要因素之一。 **根际串扰** 

+ 根际附近微生物含量高 https://doi.org/10.1016/j.scib.2020.03.005

+ 植物分泌物塑造微生物群落。总体而言，植物散发出高达 20% 的固定碳和 15% 的氮，其中包括一系列简单分子，如糖、有机酸和次级代谢物，以及复杂的聚合物，如粘液 https://doi.org/10.1016/j.tplants.2017.09.003

+ 丁香假单胞菌初次接触后不久就高度诱导了几种与防御相关的植物蛋白的分泌 https://doi.org/10.1074/jbc.M801967200

+ “局部”根系的特定微生物定植会调节“全身性”的渗出液成分。例如，枯草芽孢杆菌调节著名的酰基糖杀虫剂的全身渗出。我们还发现糖基化壬二酸是由微生物组诱导的，可能在 SIREM 中起作用。结果表明，微生物组重新编程的系统性根系分泌物促进了土壤调节，并为更深入地了解微生物群如何调节根系代谢和分泌铺平了道路。 https://doi.org/10.1073/pnas.1912130117

+ 植物通过生产固定碳资源帮助维持稳定的根际生物群落 https://doi.org/10.1007/s00253-018-9556-6

**植物会分泌分泌物影响群落，但影响机制不清楚。**

## 矩阵求导

+ 布局 分母布局 分子布局
+ 对于向量变元$x$中每一个分量都要求导
+ 梯度 对应列向量

# 2021-11-11

## PLAN
+ **根际微生物overview**
+ **introduction初稿**
+ **网申填写**
+ **RNN overview Liheng course**

## 注释方法
利用Prokka 数据库 需要再次确定

### 比较基因组方法 mauve
mauve 可视化网站 http://darlinglab.org/mauve/user-guide/viewer.html

### Steps
+ 下载序列
+ 注释为gbk
+ 可视化

## introduction

In addition to soil physical and chemical properties, plant rhizosphere exudates were also confirmed to be related to soil microbial community by previous studies. Previous studies have reported that root exudates of different arabidopsis species significantly alter the soil microbiome(Micallef et al., 2009). A phenomenon known as rhizosphere crosstalk shows that root-soil interactions resulting from the constant secretion of small molecules, including sugars and vitamins, as well as complex proteins, are the main factors that determine microbial nature(Micallef et al., 2009). Plants help maintain stable rhizosphere biomes by producing fixed carbon resources(Korenblum et al., 2020; Lucke et al., 2020). However, the mechanism by which plant roots influence microbial communities has not been revealed.

## RNN

### GRU

本质： 增加每一层的Hiddenlayer数 即从$H_t$增加至$R_t,Z_t$学习的参数量*3

R 负责短期 
Z 负责长期如果$t' 到 t $时间内Z=1则所有的则该时间段内的新状态都没有被考虑

### LSTM

更复杂 三门 二状态

# 2021-11-12

## PLAN
+ **自回归文献阅读**
+ **修改GRU模型为套路模型**

## 玻尔兹曼机

不受限情况下算后验概率利用MCMC方法 https://www.bilibili.com/video/BV1kJ41127aD?p=3

## 清洗数据

需要修改读文件部分

# 2021-11-13

## PLAN
+ **VAE 作业代码框架搭建**
+ **蛋白序列建模 总结**
+ **讨论 overview**


## VAE 项目变量解读

z_all 所有的隐变量 tf层 tf 1182

genes 11941

cells 13494

### 进度

将基因丰度表读入

### 模型搭建完毕

做超参数搭建 利用的技术包括梯度裁剪 调低学习率

**如果梯度裁剪后依然出现了nan现象直接降低学习率**

## 文章讨论

### 对于无氮肥现象

+ 现象1 氮影响根际微生物群落 已有论文1 2 3 直接讨论
+ 有膜无膜低浓度无差别 ？ 找文献证明 原因为失去迁移力 但在高氮浓度依然会有的先现象
+ 不同氮对运动细菌影响 也要找

### 三者关系

+ 植物根际诱导hitchhiking 分泌物问题 谈不谈？
+ 有无植物 氮浓度对于hitchhiking作用影响十分显著  
  

## 蛋白建模扩展

### wiki资料
https://en.wikipedia.org/wiki/Direct_coupling_analysis

### 文献
+ https://www.nature.com/articles/s41592-018-0138-4 **DeepSequece**
+ https://academic.oup.com/mbe/article/35/4/1018/4815777 **bmDCA**


# 2021-11-14

## PLAN
+ **VAE调惨 可视化等overview**
+ **蛋白序列建模deepsequence 总结**
+ **调参技巧学习**


## VAE 损失函数

Output假定为正态分布 均值为输出方差为常数 构造出mse

## 调参

如果在训练集就效果不理想要增大模型容量解决


# 2021-11-15

## PLAN
+ **网申填报 诺特丹**
+ **推荐信邮件**
+ **VAE训练问题总结**
+ **单细胞文献**

## Problems
+ tf 出现负值 (限制tf三层 具体处理？)
+ UMAP 在原始数据结果不好

## perturb-seq

原理 向细胞中转入可以是特定基因成魔的CRISPR inference RNA序列 sgRNA  沉默特定基因后 观察

### 文章符号
Tg
DOSM

# 2021-11-16

## PLAN
+ **网申收尾**
+ **单细胞论文进一步阅读，以及分类标注**
+ **单细胞论文 umap t-sne图**

## 单细胞

### 代码备注

```r
attributes(train)$dim
attributes(train[tfs, ])$dim
attributes(tpm)$dim
attributes(tpm[tfs, ])$dim

```
train 筛选出的五种细胞处理
tpm所有的
`train[tfs, ]`转录本training set
`tpm[tfs, ]` 未筛选的转录本

### umap 图

利用genes构建umap图可以将不同类别区分开但用tf做图无法分类

重点关注tf对于genes的影响

### 需解决问题

不考虑tf时重建效率不高，loss巨大

隐层构建可以考虑用scvi的构建

https://www.nature.com/articles/s41592-018-0229-2.pdf

# 2021-11-17

## PLAN
+ **展示ppt完成素材**
+ **单细胞网络调优**
+ **分析scvi代码**
+ **深度RNN**

## VAE 不考虑tf数据后恢复情况

5000左右loss 不够理想 可以加入 tf再做尝试

不用过拟合防止技术算一下

# 2021-11-18

## PLAN
+ **UA 申请系统填报**
+ **展示ppt**
+ **不用过拟合技术 计算VAE损失**
+ **宏基因组套路讨论**
+ 总环讨论

## 多利用scvi的代码为基础
```python
import os
from typing import Literal
import torch
import torch.nn as nn
import torch.nn.functional as F
from scvi.nn import FCLayers,Encoder
# import scvi
# import  scvi



class VAE(nn.Module):

    def __init__(self,n_gene=784, h_dim=[800,400], z_dim=200):
        super(VAE, self).__init__()

        # [b,784] -> [b,20]
        # u:[b,10] sigma:[b,10]
        self.use_batch_norm: Literal["encoder", "decoder", "none", "both"] = "both",
        self.use_layer_norm: Literal["encoder", "decoder", "none", "both"] = "none",
        self.n_gene=n_gene
        self.latent_distribution = "normal"
        use_batch_norm_encoder = self.use_batch_norm == "encoder" or self.use_batch_norm == "both"
        use_layer_norm_encoder = self.use_layer_norm == "encoder" or self.use_layer_norm == "both"
        self.encoder = nn.Sequential(
            nn.Linear(n_gene, h_dim[0]),
            nn.ReLU(),
            nn.Linear(h_dim[0], h_dim[1]),
            nn.ReLU(),
            nn.Linear(h_dim[1], z_dim*2),
            nn.ReLU(),
        )
        # [b,20] -> [b,784]
        self.decoder = FCLayers(
            n_in=200,
            n_out=800,
            n_cat_list=None,
            n_layers=1,
            n_hidden=400,
            dropout_rate=0.2
        )

        self.z_encoder = Encoder(
            n_gene,
            400,
            n_layers=2,
            n_hidden=800,
            dropout_rate=0.2,
            distribution=self.latent_distribution,
            use_batch_norm=use_batch_norm_encoder,
            use_layer_norm=use_layer_norm_encoder,
            activation_fn=torch.nn.LeakyReLU,
        )
        self.criterion = nn.MSELoss()
        self.linear_out = nn.Linear(800, self.n_gene)

    def forward(self, x):
        batch_size = x.size(0)
        # flatten
        # x = x.view(batch_size, 784)
        # encoder
        # [b,20] , including mean and sigma
        h_ = self.encoder(x)
        # [b,20] -> [b,10] and [b,10]
        mu, sigma = h_.chunk(2, dim=1)
        # reparametrize trick, epison~N(0,1)
        h = mu + sigma * torch.randn_like(sigma)
        # decoder
        x_hat = self.decoder(h)
        x_hat = self.linear_out(x_hat)
        x_hat = F.relu(x_hat)
        # reshape
        # x_hat = x_hat.view(batch_size, 1, 28, 28)

        # kl divergence
        kld = 0.5 * torch.sum(
            torch.pow(mu, 2) +
            torch.pow(sigma, 2) -
            torch.log(1e-8 + torch.pow(sigma, 2)) - 1
        ) / (batch_size*self.n_gene)

        return x_hat, kld

class vae_loss(nn.Module):
    def __init__(self):
        super().__init__()
        
    def calculate(self, x, x_reconst , mu ,log_var):
        reconst_loss = F.mse_loss(x_reconst, x, size_average=False)
        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
        return reconst_loss , kl_div

    def forward(self, x, x_reconst , mu ,log_var):
        reconst_loss , kl_div = self.calculate(x, x_reconst , mu ,log_var)
        return reconst_loss + kl_div

```

## 宏基因组参考文献

氮代谢 Metagenomic reconstruction of nitrogen cycling pathways in a CO2-
enriched grassland ecosystem


# 2021-11-19

## PLAN
+ **阅读宏基因组论文**
+ **调参**
+ **adata 数据结构熟悉**
+ **讨论+结果确定基调**

## scanpy 数据结构总结

### 对于该项目
尽量使用read_csv后手动设置基因名称，细胞名称等内容

## VAE调参
此前的问题，没有进行cell normalization 就进行训练了，如果进行cell normalization后进行训练会效果好一些


## 功能

一定要做成表

### 无植物
有膜 无差异 cell motility
无膜 有差异 cell motility

### 有植物
有膜 无差异 cell motility
无膜 有差异 cell motility

## 讨论
无植物 无运动菌hitchhiking  无bacillus

## 无植物微生物结论基调

### 微生物不同氮肥浓度
**H1-F-vs-H2-F-vs-H3-F 0.8601 0.006 ** 无差异**
H1-NF-vs-H2-NF-vs-H3-NF 0.3333 0.026 *


# 2021-11-20

## PLAN
+ **复现vega同等量级loss**
+ **宏基因组多篇文章混合整理**

## 训练记录

利用vega的pmbc数据集训练问题不大，加上tfloss重新训练并作出 loss map进行观察，可能这个数据有一定问题

可以用scanpy进行更进一步的数据预处理

## 文章

### 2021年applied

+ rda pca 找到不同处理的群落之间区别
+ 找到氮代谢基因并分析
+ binning后每一个mag都与氮代谢进行对应

### 2021 fim

+ 16 s做物种堆叠图
+ 注释找到 gene
+ gene做丰度变化分析和富集分析基于kegg 
+ 做出gene之间的网络并做出模块划分

# 2021-11-21

## PLAN

+ **教堂山硕士**
+ **杨老师推荐信**
+ **宏基因组第三篇文献**
+ **KAUST硕士填报**

## 大流行的影响 文件
+ 英语考试
+ 上了网课
+ 心情一开始不好，导致身体不健康 但克服了
+ 由于从事计算生物学研究不会让我受到很大的学术影响，和老师的见面都改成了网课

After the outbreak of COVID-19, it had a great impact on my life at the beginning. Many English exams, such as IETLS and GRE, were stopped, which had a great impact on me. At the same time, in China, we cannot go to the school to have offline classes, so we need to take online classes, which has affected the teaching effect. Most of the time, during the break, we would ask the teacher about some knowledge points we did not understand during the teaching process. However, in the process of online courses, it is sometimes difficult to solve problems only through text communication. At the beginning, we felt too shy to speak in online communication, but we gradually adapted to this form later.

In addition, during the pandemic, I felt it was a blow to students' psychology, and the lack of exercise at home made me very depressed. When the epidemic gradually improved, I created conditions to go outdoors to do a good job of running and exercise, which helped me to improve. Exercise is a good way for me to release pressure.

Academically, the pandemic has made it difficult for me to communicate with my advisor. In offline research internships, we can immediately communicate with advisors when we have a problem. During the pandemic, this becomes less possible. But with the spread of web conferencing, we can also solve many problems in scientific research through online video conferencing. Especially considering that I majored in computational biology, most of my work was carried out using servers without field experiments, which made my scientific research work less affected.


## 宏基因组文章3

### 主线逻辑
+ 找到优势菌
+ 分析进化树
+ 找到N基因丰度变化 & 氨基酸合成变化

### 方法

KO注释找到基因 unigene

# 2021-11-22

## PLAN
+ **杨老师推荐信完善**
+ **使用scanpy 初步分析数据**
+ **宏基因组项目数据查找**

## 宏基因组方法
Unigenes_abundance_keggFunc.tsv
文件包含着gene描述 以及丰度

## umap 数据
无法每一类聚集而Kang数据可以复现

# 2021-11-23

## PLAN
+ **UA ps**
+ **UNC ps**
+ **ND ps**

# 2021-11-24

## PLAN
+ **甲基化pipeline 完成**
+ **申请情况总结**

## 申请情况总结

### Madison
材料完成，等待老板给材料提意见以及推荐信
需要修改 **diversity statement**

### Penn State
等待Karthik推荐信以及等待ps内容可以填报

### Arizona
系统未开

### Tulane
等待润色ps 推荐信未上传

### ND
推荐信上传两篇，ps在修改，covid-ps还未修改

### UNC
ps在修改，推荐信未上传

### CMU
ps 没写 推荐信未上传 ddl12.9

## 配置ltr_finder

```bash
bash /home/ubuntu/Arabidopsis/Scripts/NAR_LTR_Finding_Code/find_LTRs.sh $1
# 在其中要保证其是fasta文件的.前部分如 chr1.fasta $1 为chr1 
```

# 2021-11-25

## PLAN
+ **ltr_finder 修复**
+ **杨老师推荐信提交**
+ **论文结果讨论完成**

## 论文中代码指导(NAR)
```txt
This folder contains scripts used to generate, process and visualize plant 
LTR retrotransposon sequences, as well as triplexes and quadruplexes in them

Detecting and annotating LTR retroelements
------------------------------------------
1. find_LTRs.sh
2. add_features.pl
(the feature used in the paper was the Blast-GD annotation line produced 
by running tblastx on the retrotransposon sequence against a collection 
of GAG, RT, INT and other retrotransposon protein domains)
3. example.ltr	#example output from find_LTRs.sh

Filtration of detected sequences
--------------------------------
4. select_best.pl	#Selects the best LTR element for each sequence analysed
(cat example.ltr | ./select_best.pl > example.ltr)
5. guess_orientation.pl
(cat example_best.ltr |  ./guess_orientation.pl | egrep -v " N" | uniq > example_best.orientation
6. process_orientation.pl	#Reverse sequences where needed
(cat example_best.orientation | ./process_orientation.pl > example_processed.fa)

Source sequence file
--------------------
(macas_clean.fa, FASTA file not included)

Quadruplex detection
--------------------
8. quad.pl, quad2.pl	#Run regular expressions for PQS2, PQS3 on FASTA file
(cat macas_clean.fa | ./quad_fix.pl > quad_macas_fix_1.out)
(cat macas_clean.fa | ./quad2_fix.pl > quad_macas_fix_2.out)

Triplex_detection
-----------------
9. get_triplexes.R

Intermediate data tables
------------------------
macas_clean_proms.tab
best_ltrs.tab
quad_macas_1c_fix.tab
quad_macas_1g_fix.tab
quad_macas_2c_fix.tab
quad_macas_2g_fix.tab
macas_clean_18_best.tab

Visualization
-------------
10. load_data_fix.R	#load data into R variables
11. summary_fig.R	#example visualization
```


# 11.26 度过了美好的感恩节假期

# 2021-11-27

## PLAN
+ **机器学习课程设计**
+ 分子生物学作业
+ CMU ps修改


# 2021-11-28

## PLAN
+ **机器学习课程设计**
+ **CMU ps**
+ 百面生成模型
+ NLP